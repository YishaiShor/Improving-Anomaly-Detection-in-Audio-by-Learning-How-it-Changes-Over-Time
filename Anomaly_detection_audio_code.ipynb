{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Improving Anomaly Detection in Audio by Learning How it Flows in Time - Yishai Shor \"\"\"\n",
        "\n",
        "#########################################\n",
        "########## Import Libraries #############\n",
        "#########################################\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Packages for loading and visualizing audio files\n",
        "import librosa\n",
        "import librosa.display\n",
        "import os\n",
        "\n",
        "# Packages for deep learning processing\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization, Reshape, Input, InputLayer, GlobalAveragePooling3D\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "\n",
        "\n",
        "#########################################\n",
        "########## Defining functions ###########\n",
        "#########################################\n",
        "def load_audio(kind):\n",
        "    audio_paths = []\n",
        "    if kind == 'train':\n",
        "        audio_paths.append(\"/content/google_colab/train1\")\n",
        "    elif kind == 'test':\n",
        "        audio_paths.extend((\"/content/google_colab/test_normal1\", \"/content/google_colab/test_anomaly1\"))\n",
        "\n",
        "    # Convert the audio waveform to spectrogram\n",
        "    Xdb = []\n",
        "    for audio_path in audio_paths:\n",
        "        audio_clips = os.listdir(audio_path)\n",
        "        for audio_clip in audio_clips:\n",
        "            x, sr = librosa.load(os.path.join(audio_path, audio_clip), sr=41000)\n",
        "            X = librosa.stft(x)\n",
        "            Xdb.append(librosa.amplitude_to_db(abs(X)))\n",
        "    return Xdb\n",
        "\n",
        "\n",
        "def create_regular_x(single_x):\n",
        "    groups = len(single_x) // 41  # 25 groups\n",
        "    X_temp = [single_x[groups*i:groups*(i+1)] for i in range(groups)]\n",
        "    X_run_1 = [X_temp[i-2:i+3] for i in range(2, groups-2)]\n",
        "    return X_run_1\n",
        "\n",
        "\n",
        "def create_twice_fast_x(single_x):\n",
        "    groups = len(single_x) // 41  # 25 groups\n",
        "    X_temp = [single_x[groups*i:groups*(i+1)] for i in range(groups)]\n",
        "    X_run_2 = [[X_temp[i-4], X_temp[i-1], X_temp[i], X_temp[i+1], X_temp[i+4]] for i in range(4, groups-4)]\n",
        "    return X_run_2\n",
        "\n",
        "\n",
        "def create_oposite_order_x(single_x):\n",
        "    groups = len(single_x) // 41  # 25 groups\n",
        "    X_temp = [single_x[groups*i:groups*(i+1)] for i in range(groups)]\n",
        "    X_run_3 = [[X_temp[i+2],X_temp[i+1],X_temp[i],X_temp[i-1],X_temp[i-2]] for i in range(2, groups-2)]\n",
        "    return X_run_3\n",
        "\n",
        "\n",
        "def create_without_i(single_x):\n",
        "    groups = len(single_x) // 41  # 25 groups\n",
        "    X_temp = [single_x[groups*i:groups*(i+1)] for i in range(groups)]\n",
        "    X_run_1 = [[X_temp[i-2], X_temp[i-1], X_temp[i+1], X_temp[i+2]] for i in range(2, groups-2)]\n",
        "    Y_run_1 = [X_temp[i] for i in range(2, groups-2)]\n",
        "    return X_run_1, Y_run_1\n",
        "\n",
        "def create_labels(length, label_kind):\n",
        "    return length*[label_kind]\n",
        "\n",
        "\n",
        "def create_data_for_run(model_num, kind):\n",
        "    X_train = load_audio(kind)\n",
        "    X_train_regular = [create_regular_x(x) for x in X_train]\n",
        "    X_train_twice_fast = [create_twice_fast_x(x) for x in X_train]\n",
        "    X_train_opposite_order  = [create_oposite_order_x(x) for x in X_train]\n",
        "    X_train_without_i, Y_train_without_i = [create_without_i(x) for x in X_train]\n",
        "\n",
        "    if model_num != 3 and kind == 'train':\n",
        "        if model_num == 1:\n",
        "            x_validate = X_train_opposite_order\n",
        "        elif model_num == 2:\n",
        "            x_validate = X_train_twice_fast\n",
        "\n",
        "        # create labels\n",
        "        x_train = [item for sublist in X_train_regular for item in sublist]\n",
        "        y_train = create_labels(len(x_train), 1)\n",
        "        x_train += [item for sublist in x_validate for item in sublist]\n",
        "        y_train += create_labels(len(x_train)-len(y_train), 0)\n",
        "\n",
        "    elif model_num == 3 and kind == 'train':\n",
        "        x_train = [item for sublist in X_train_without_i for item in sublist]\n",
        "        y_train = [item for sublist in Y_train_without_i for item in sublist]\n",
        "\n",
        "    elif kind == 'test' and model_num != 3:\n",
        "        x_train = [item for sublist in X_train_regular for item in sublist]\n",
        "        count_regular = len(x_train) // 2\n",
        "        y_train = create_labels(count_regular, 1) + create_labels(count_regular, 0)\n",
        "\n",
        "    elif kind == 'test' and model_num == 3:\n",
        "        x_train = [item for sublist in X_train_without_i for item in sublist]\n",
        "        y_train = [item for sublist in Y_train_without_i for item in sublist]\n",
        "\n",
        "    x_train = np.asarray(x_train).astype('float32')\n",
        "    y_train = np.asarray(y_train).astype('float32')\n",
        "\n",
        "    del X_train_regular, X_train_twice_fast, X_train_opposite_order, X_train, Y_train_without_i, X_train_without_i\n",
        "\n",
        "    return x_train, y_train\n",
        "\n",
        "\n",
        "def anomaly_decision(prediction,threshold):\n",
        "    model_prediction = []\n",
        "    for i in range(0, len(prediction)-20, 21):\n",
        "        temp = sorted(prediction[i:i+21])\n",
        "        if sum(temp[:3]) < threshold:\n",
        "            model_prediction.append(0)\n",
        "        else:\n",
        "            model_prediction.append(1)\n",
        "    return model_prediction\n",
        "\n",
        "\n",
        "def anomaly_decision_model_3(mae_prediction, limit=2):\n",
        "    mean_mae_train = np.mean(mae_prediction)\n",
        "    sd_mae_train = np.std(mae_prediction)\n",
        "    decision = np.where(np.abs(mae_prediction - mean_mae_train) / sd_mae_train < limit, 1, 0)\n",
        "    return decision.tolist()\n",
        "\n",
        "\n",
        "def test_observation_min_value(new_list):\n",
        "    model_prediction = [round(min(new_list[i:i+21])) for i in range(0, len(new_list)-20, 21)]\n",
        "    return model_prediction\n",
        "\n",
        "\n",
        "def accuracy(list1, list2):\n",
        "    return 1 - sum(abs(x - y) for x, y in zip(list1, list2)) / len(list1)\n",
        "\n",
        "def compute_mae(list1, list2):\n",
        "    mae = tf.keras.losses.MeanAbsoluteError()\n",
        "    model_mae = []\n",
        "    list1_ok = []\n",
        "    list2_ok = []\n",
        "\n",
        "    # Creating lists of subsequences\n",
        "    for temp_old_list in [list1, list2]:\n",
        "        for i in range(0, len(temp_old_list) - 20, 21):\n",
        "            temp = temp_old_list[i:i + 21]\n",
        "            if temp_old_list is list1:\n",
        "                list1_ok.append(temp)\n",
        "            else:\n",
        "                list2_ok.append(temp)\n",
        "\n",
        "    # Compute MAE for each sequence\n",
        "    for x, y in zip(list1_ok, list2_ok):\n",
        "        model_mae.append(mae(x, y).numpy())\n",
        "\n",
        "    return model_mae\n",
        "\n",
        "def apply_model():\n",
        "    # Create the model\n",
        "    model = Sequential()\n",
        "    model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(MaxPooling3D(pool_size=(1,2,2)))\n",
        "    model.add(BatchNormalization(center=True, scale=True))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv3D(64, kernel_size=(2,2,2), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(MaxPooling3D(pool_size=(1,2,2)))\n",
        "    model.add(BatchNormalization(center=True, scale=True))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv3D(128, kernel_size=(2,2,2), activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
        "    model.add(BatchNormalization(center=True, scale=True))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(36, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(24, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def build_autoencoder(audio_file_shape, code_size):\n",
        "    # The encoder\n",
        "    encoder = Sequential()\n",
        "    encoder.add(Conv3D(32, kernel_size=(1, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=audio_file_shape))\n",
        "    encoder.add(Dropout(0.5))\n",
        "    encoder.add(Conv3D(32, kernel_size=(1, 2, 2), activation='relu', kernel_initializer='he_uniform'))\n",
        "    encoder.add(Dropout(0.5))\n",
        "    encoder.add(GlobalAveragePooling3D())\n",
        "    encoder.add(Dense(code_size))\n",
        "\n",
        "    # The decoder\n",
        "    decoder = Sequential()\n",
        "    decoder.add(Dense(audio_file_shape[1] * audio_file_shape[2] * audio_file_shape[3], input_shape=(code_size,)))\n",
        "    decoder.add(Reshape((audio_file_shape[1], audio_file_shape[2], audio_file_shape[3])))\n",
        "    decoder.add(Conv3DTranspose(32, kernel_size=(1, 2, 2), activation='relu', kernel_initializer='he_uniform'))\n",
        "    decoder.add(Dropout(0.5))\n",
        "    decoder.add(Conv3DTranspose(32, kernel_size=(1, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
        "    decoder.add(Dropout(0.5))\n",
        "    decoder.add(Conv3DTranspose(1, kernel_size=(1, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
        "\n",
        "    return encoder, decoder\n",
        "\n",
        "#### Evaluating Models ####\n",
        "\n",
        "########################\n",
        "####### Model 1 ########\n",
        "########################\n",
        "# Fit data to model 1\n",
        "x_train, y_train = create_data_for_run(1,'train')\n",
        "model = apply_model()\n",
        "model.build(input_shape=(None,5,25,431,1))\n",
        "model.summary()\n",
        "history_1 = model.fit(x_train, y_train, batch_size=60, epochs=90, verbose=1, validation_split=0.1)\n",
        "del x_train, y_train\n",
        "\n",
        "# Resalts\n",
        "x_test, y_test = create_data_for_run(1, 'test')\n",
        "predictions_1_model = model.predict(x_test)\n",
        "predictions_1_model_cons = anomaly_decision(predictions_1_model, 0.1)\n",
        "y_test = test_observation_min_value(y_test)\n",
        "\n",
        "########################\n",
        "####### Model 2 ########\n",
        "########################\n",
        "\n",
        "# Fit data to model 2\n",
        "x_train, y_train = create_data_for_run(2,'train')\n",
        "model = apply_model()\n",
        "model.build(input_shape=(None,5,25,431,1))\n",
        "model.summary()\n",
        "history_2 = model.fit(x_train, y_train, batch_size=60, epochs=90, verbose=1, validation_split=0.1)\n",
        "del x_train, y_train\n",
        "\n",
        "# cheking resalts\n",
        "x_test, y_test = create_data_for_run(2, 'test')\n",
        "predictions_2_model = model.predict(x_test)\n",
        "predictions_2_model_decision = anomaly_decision(predictions_2_model, 0.1)\n",
        "y_test = test_observation_min_value(y_test)\n",
        "\n",
        "########################\n",
        "####### Model 3 ########\n",
        "########################\n",
        "# Create train data\n",
        "x_train, y_train = create_data_for_run(3, 'train')  # x_train and y_train are created using the 'create_data_for_run' function\n",
        "audio_file_shape = (4, 25, 801)  # Shape of the audio file\n",
        "\n",
        "# Build Autoencoder\n",
        "encoder, decoder = build_autoencoder(audio_file_shape, 1000)  # Encoder and decoder models are built\n",
        "inp = Input((4, 25, 801, 1))  # Define input shape\n",
        "code = encoder(inp)  # Encode the input\n",
        "reconstruction = decoder(code)  # Reconstruct the input\n",
        "\n",
        "autoencoder = Model(inp, reconstruction)  # Create the autoencoder model\n",
        "autoencoder.compile(optimizer='adamax', loss='mae')  # Compile the model with optimizer and loss function\n",
        "\n",
        "history_3 = autoencoder.fit(x=x_train, y=y_train, epochs=20, validation_split=0.1)  # Train the autoencoder\n",
        "MAE_model = history_3.history['loss']  # Get the loss values from training history\n",
        "MAE = MAE_model[-1]  # Get the last MAE value\n",
        "del x_train, y_train  # Delete variables to save memory\n",
        "\n",
        "# Testing\n",
        "x_test, y_test = create_data_for_run(3, 'test')  # Create test data using the 'create_data_for_run' function\n",
        "predictions_3_method = autoencoder.predict(x_test)  # Predict using the autoencoder model\n",
        "\n",
        "# Compute the MAE of each picture\n",
        "predictions_3_method = np.reshape(predictions_3_method, [8400, 25, 801])  # Reshape predictions to match the target shape\n",
        "predictions_3_method.tolist()  # Convert predictions to a Python list\n",
        "y_test.tolist()  # Convert y_test to a Python list\n",
        "mae_test = compute_mae(predictions_3_method, y_test)  # Compute the MAE between predictions and y_test\n"
      ],
      "metadata": {
        "id": "zsH24U2HGPHg"
      },
      "execution_count": 4,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}