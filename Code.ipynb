{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3SSdn2jDTIav"
   },
   "outputs": [],
   "source": [
    "\"\"\" Improving Anomaly Detection in Audio by Learning How it Flows in Time - Yishai Shor\n",
    "           \"\"\"\n",
    "\n",
    "#########################################\n",
    "########## Import Libraries #############\n",
    "#########################################\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "\n",
    "# for loading and visualizing audio files\n",
    "import librosa\n",
    "import librosa.display\n",
    "import wave, os, glob\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Conv3D, Conv2D, MaxPooling3D, MaxPooling2D, Dropout, BatchNormalization, \\\n",
    "    Reshape, Input, InputLayer, GlobalAveragePooling1D, GlobalAveragePooling3D\n",
    "import tensorflow\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#########################################\n",
    "########## Defining functions ###########\n",
    "#########################################\n",
    "\n",
    "def load_audio(kind):\n",
    "    # Step 1: Define path\n",
    "    if kind == 'train':\n",
    "        audio_fpath = \"/content/google_colab/train1\"\n",
    "        audio_clips = os.listdir(audio_fpath)\n",
    "        zero = []\n",
    "        for i in range(0, len(audio_clips)):\n",
    "            x, sr = librosa.load(audio_fpath + '/' + audio_clips[i], sr=41000)\n",
    "            zero.append(x)\n",
    "    elif kind == 'test':\n",
    "        audio_fpath = \"/content/google_colab/test_normal1\"\n",
    "        audio_clips = os.listdir(audio_fpath)\n",
    "        zero = []\n",
    "        for i in range(0, len(audio_clips)):\n",
    "            x, sr = librosa.load(audio_fpath + '/' + audio_clips[i], sr=41000)\n",
    "            zero.append(x)\n",
    "        audio_fpath = \"/content/google_colab/test_anomaly1\"\n",
    "        audio_clips = os.listdir(audio_fpath)\n",
    "        for i in range(0, len(audio_clips)):\n",
    "            x, sr = librosa.load(audio_fpath + '/' + audio_clips[i], sr=41000)\n",
    "            zero.append(x)\n",
    "    # Step 2: Convert the audio waveform to spectrogram\n",
    "    Xdb = []\n",
    "    for i in range(0,len(zero)):\n",
    "        X = librosa.stft(zero[i])\n",
    "        Xdb.append(librosa.amplitude_to_db(abs(X)))\n",
    "        print(i)\n",
    "    return Xdb\n",
    "\n",
    "  \n",
    "def create_regular_x(single_x):\n",
    "    groups = int(len(single_x)/41) # 25 groups\n",
    "    X_temp = [0]*(groups)\n",
    "    for i in range(0,groups,1):\n",
    "        X_temp[i] = single_x[groups*(i):groups*(i+1)]\n",
    "    X_run_1 = [0]*(groups-4)\n",
    "    for i in range(2,groups-2,1):\n",
    "        X_run_1[i-2] = [X_temp[i-2],X_temp[i-1],X_temp[i],X_temp[i+1],X_temp[i+2]]\n",
    "    return X_run_1\n",
    "\n",
    "def create_twice_fast_x (single_x):\n",
    "    groups = int(len(single_x)/41) # 25 groups\n",
    "    X_temp = [0]*(groups)\n",
    "    for i in range(0,groups,1):\n",
    "        X_temp[i-2] = single_x[groups*(i):groups*(i+1)]\n",
    "    #print(len(X_temp))\n",
    "    X_run_2 = [0]*(groups-8)\n",
    "    for i in range(4,groups-4,1):\n",
    "        X_run_2[i-4] = [X_temp[i-4],X_temp[i-1],X_temp[i],X_temp[i+1],X_temp[i+4]]\n",
    "    return X_run_2\n",
    "\n",
    "def create_oposite_order_x(single_x):\n",
    "    groups = int(len(single_x)/41) # 25 groups\n",
    "    X_temp = [0]*(groups)\n",
    "    for i in range(0,groups,1):\n",
    "        X_temp[i-2] = single_x[groups*(i):groups*(i+1)]\n",
    "    X_run_3 = [0]*(groups-4)\n",
    "    for i in range(2,groups-2,1):\n",
    "        X_run_3[i-2] = [X_temp[i+2],X_temp[i+1],X_temp[i],X_temp[i-1],X_temp[i-2]]\n",
    "    return X_run_3\n",
    "\n",
    "def create_without_i(single_x):\n",
    "    groups = int(len(single_x)/41) # 25 groups\n",
    "    X_temp = [0]*(groups)\n",
    "    for i in range(0,groups,1):\n",
    "        X_temp[i] = single_x[groups*(i):groups*(i+1)]\n",
    "    X_run_1 = [0]*(groups-4)\n",
    "    Y_run_1 = [0]*(groups-4)\n",
    "    for i in range(2,groups-2,1):\n",
    "        X_run_1[i-2] = [X_temp[i-2],X_temp[i-1],X_temp[i+1],X_temp[i+2]]\n",
    "        Y_run_1[i-2] = X_temp[i]\n",
    "    return X_run_1, Y_run_1\n",
    "\n",
    "def create_lables(length, label_kind):\n",
    "    return length*[label_kind]\n",
    "\n",
    "def create_data_for_run(model_num, kind):\n",
    "    X_train = load_audio(kind)\n",
    "    X_train_regular = []\n",
    "    X_train_twice_fast = []\n",
    "    X_train_oposite_order = []\n",
    "    X_train_without_i = []\n",
    "    Y_train_without_i = []\n",
    "    for i in range(0,len(X_train)):\n",
    "        X_train_regular.append(create_regular_x(X_train[i]))\n",
    "        X_train_twice_fast.append(create_twice_fast_x(X_train[i]))\n",
    "        X_train_oposite_order.append(create_oposite_order_x(X_train[i]))\n",
    "        x,y = create_without_i(X_train[i])\n",
    "        X_train_without_i.append(x)\n",
    "        Y_train_without_i.append(y)\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    count_regular = 0\n",
    "    count_revrse = 0\n",
    "    if model_num != 3 and kind == 'train':\n",
    "        if model_num == 1:\n",
    "            x_validate = X_train_oposite_order\n",
    "        elif model_num == 2:\n",
    "            x_validate = X_train_twice_fast\n",
    "        for i in range(0,len(X_train_regular)):\n",
    "            for j in range(0,len(X_train_regular[i])):\n",
    "                x_train.append(X_train_regular[i][j])\n",
    "                count_regular += 1\n",
    "        for i in range(0,len(x_validate)):\n",
    "            for j in range(0,len(x_validate[i])):\n",
    "                x_train.append(x_validate[i][j])\n",
    "                count_revrse += 1\n",
    "        y_train = create_lables(count_regular,1)\n",
    "        y_temp = create_lables(count_revrse,0)\n",
    "        y_train = y_train[:] + y_temp[:]\n",
    "    elif model_num == 3 and kind == 'train':\n",
    "        for i in range(0,len(X_train_without_i)):\n",
    "            for j in range(0,len(X_train_without_i[i])):\n",
    "                x_train.append(X_train_without_i[i][j])\n",
    "                y_train.append(Y_train_without_i[i][j])\n",
    "    elif kind == 'test' and model_num != 3:\n",
    "        for i in range(0,len(X_train_regular)):\n",
    "            for j in range(0,len(X_train_regular[i])):\n",
    "                x_train.append(X_train_regular[i][j])\n",
    "                count_regular += 1\n",
    "        count_regular = int(count_regular/2)\n",
    "        y_train = create_lables(count_regular,1) # test normal and abnormal groups are of the same size and even\n",
    "        y_temp = create_lables(count_regular,0)\n",
    "        y_train = y_train[:] + y_temp[:]\n",
    "    elif kind == 'test' and model_num == 3:\n",
    "        for i in range(0,len(X_train_without_i)):\n",
    "            for j in range(0,len(X_train_without_i[i])):\n",
    "                x_train.append(X_train_without_i[i][j])\n",
    "                y_train.append(Y_train_without_i[i][j])\n",
    "    x_train = np.asarray(x_train).astype('float32')\n",
    "    y_train = np.asarray(y_train).astype('float32')\n",
    "    del X_train_regular, X_train_twice_fast, X_train_oposite_order, X_train, Y_train_without_i, X_train_without_i\n",
    "    return x_train, y_train\n",
    "\n",
    "def anomaly_decision(prediction):\n",
    "    # stracture data issue\n",
    "    classify_value = []\n",
    "    model_prediction = []\n",
    "    for i in range(0,len(prediction)):\n",
    "        classify_value.append(prediction[i][0])\n",
    "    # anomaly decision\n",
    "    for i in range(0,len(classify_value)-20, 21):\n",
    "        temp = classify_value[i:i+21]\n",
    "        min_val_1 = sorted(temp)[0]\n",
    "        min_val_2 = sorted(temp)[1]\n",
    "        min_val_3 = sorted(temp)[2]\n",
    "        if (min_val_1+min_val_2+min_val_3)< 0.1:\n",
    "            model_prediction.append(0)\n",
    "        else:\n",
    "            model_prediction.append(1)\n",
    "    return model_prediction\n",
    "\n",
    "def anomaly_decision_model_3(mse_prediction,mae_train = 10.6, sd_train = 0.2722 ,limit = 2):\n",
    "    decision = []\n",
    "    for i in range(0,len(mse_prediction)):\n",
    "        Z = abs(mse_prediction[i] - mae_train)/sd_train\n",
    "        if Z < limit:\n",
    "            decision.append(1)\n",
    "        else:\n",
    "            decision.append(0)\n",
    "    return decision\n",
    "\n",
    "def test_reduce_size(new_list):\n",
    "    model_prediction = []\n",
    "    for i in range(0,len(new_list)-20, 21):\n",
    "        temp = round(min(new_list[i:i+21]))\n",
    "        model_prediction.append(temp)\n",
    "    return model_prediction\n",
    "\n",
    "def accuracy(list1, list2):\n",
    "    sum1 = 0\n",
    "    for i in range(0,len(list1)):\n",
    "        sum1 += abs(list1[i]-list2[i])\n",
    "    return 1 -sum1/len(list1)\n",
    "\n",
    "def compute_mae(list1,list2):\n",
    "    from tensorflow.keras.losses import MeanAbsoluteError\n",
    "    mae = tensorflow.keras.losses.MeanAbsoluteError()\n",
    "    model_mae = []\n",
    "    list1_ok =[]\n",
    "    list2_ok = []\n",
    "    old_lists = [list1,list2]\n",
    "    new_lists = [list1_ok,list2_ok]\n",
    "    #creating list unifies by obs\n",
    "    for j in [0,1]:\n",
    "        temp_old_lists = old_lists[j]\n",
    "        temp_new_lists = new_lists[j]\n",
    "        for i in range(0,len(list1)-20, 21):\n",
    "            temp = temp_old_lists[i:i+21]\n",
    "            temp_new_lists.append(temp)\n",
    "    # compute mse of each image\n",
    "    for i in range(0,len(list1_ok)):\n",
    "        model_mae.append(mae(list1_ok[i],list2_ok[i]).numpy())\n",
    "    return model_mae\n",
    "    \n",
    "def apply_model():\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling3D(pool_size=(1,2,2)))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv3D(64, kernel_size=(2,2,2), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling3D(pool_size=(1,2,2)))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv3D(128, kernel_size=(2,2,2), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n",
    "    model.add(BatchNormalization(center=True, scale=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(48, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(36, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(24, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbAgw8rlkCeL"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "####### Model 1 ########\n",
    "########################\n",
    "# Fit data to model 1\n",
    "x_train, y_train = create_data_for_run(1,'train')\n",
    "model = apply_model()\n",
    "model.build(input_shape=(None,5,25,431,1))\n",
    "model.summary()\n",
    "history_1 = model.fit(x_train, y_train, batch_size=60, epochs=90, verbose=1, validation_split=0.1)\n",
    "del x_train, y_train\n",
    "\n",
    "# cheking resalts\n",
    "x_test, y_test = create_data_for_run(1, 'test')\n",
    "predictions_1_model = model.predict(x_test)\n",
    "predictions_1_model_cons = anomaly_decision(predictions_1_model, 1,'conservative')\n",
    "predictions_1_model_liberal = anomaly_decision(predictions_1_model, 1,'liberal')\n",
    "\n",
    "y_test = test_reduce_size(y_test)\n",
    "Test_vector = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_NBsyYeCG6A"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "####### Model 2 ########\n",
    "########################\n",
    "\n",
    "# Fit data to model 2\n",
    "x_train, y_train = create_data_for_run(2,'train')\n",
    "model = apply_model()\n",
    "model.build(input_shape=(None,5,25,431,1))\n",
    "model.summary()\n",
    "history_2 = model.fit(x_train, y_train, batch_size=60, epochs=90, verbose=1, validation_split=0.1)\n",
    "del x_train, y_train\n",
    "\n",
    "# cheking resalts\n",
    "x_test, y_test = create_data_for_run(2, 'test')\n",
    "predictions_2_model = model.predict(x_test)\n",
    "predictions_2_model_decision = anomaly_decision(predictions_2_model)\n",
    "\n",
    "y_test = test_reduce_size(y_test)\n",
    "Test_vector = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IowvHl2yWovy"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "####### Model 3 ########\n",
    "########################\n",
    "x_train, y_train = create_data_for_run(3, 'train')\n",
    "audio_file_shape = (4,25,801)\n",
    "\n",
    "def build_autoencoder(audio_file_shape, code_size):\n",
    "    # The encoder\n",
    "    encoder = Sequential()\n",
    "    # encoder.add(InputLayer(None,4,25,801,1))\n",
    "    encoder.add(Conv3D(32, kernel_size=(1,3,3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    encoder.add(Dropout(0.5))\n",
    "    encoder.add(Conv3D(32, kernel_size=(1,2,2), activation='relu', kernel_initializer='he_uniform'))\n",
    "    encoder.add(Dropout(0.5))\n",
    "    encoder.add(GlobalAveragePooling3D())\n",
    "    encoder.add(Dense(25*801))\n",
    "    #encoder.add(Dense(code_size))\n",
    "    # The decoder\n",
    "    decoder = Sequential()\n",
    "    decoder.add(InputLayer((25 * 801)))\n",
    "    # decoder.add(Flatten())\n",
    "    # decoder.add(Dense(25*801))\n",
    "    decoder.add(Reshape((25, 801, 1)))\n",
    "    return encoder, decoder\n",
    "\n",
    "# Run Model\n",
    "encoder, decoder = build_autoencoder((4, 25, 801), 1000)\n",
    "inp = Input((4, 25, 801, 1))\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "\n",
    "autoencoder = Model(inp,reconstruction)\n",
    "autoencoder.compile(optimizer='adamax', loss='mae')\n",
    "print(autoencoder.summary())\n",
    "\n",
    "history_3 = autoencoder.fit(x=x_train, y=y_train, epochs=20, validation_split=0.1)\n",
    "MAE_model = history_3.history['loss']\n",
    "MAE = MAE_model[-1]\n",
    "del x_train, y_train\n",
    "\n",
    "# Testing\n",
    "x_test, y_test = create_data_for_run(3, 'test')\n",
    "predictions_3_method = autoencoder.predict(x_test)\n",
    "\n",
    "# compute the mae of each picture\n",
    "predictions_3_method = np.reshape(predictions_3_method,[8400,25,801])\n",
    "predictions_3_method.tolist()\n",
    "y_test.tolist()\n",
    "mae_test = compute_mae(predictions_3_method,y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
